{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "03-LDA-NMF-Assessment-Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-Asgar-Lakdawala/NLP/blob/main/03_LDA_NMF_Assessment_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45ESeC0b3NxM"
      },
      "source": [
        "___\n",
        "\n",
        "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Q9ZV5F-M3NxT"
      },
      "source": [
        "# Topic Modeling Assessment Project\n",
        "\n",
        "Welcome to your Topic Modeling Assessment! For this project you will be working with a dataset of over 400,000 quora questions that have no labeled cateogry, and attempting to find 20 cateogries to assign these questions to. The .csv file of these text questions can be found underneath the Topic-Modeling folder.\n",
        "\n",
        "Remember you can always check the solutions notebook and video lecture for any questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avPtEz413NxU"
      },
      "source": [
        "#### Task: Import pandas and read in the quora_questions.csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "-yvnKaD83NxV"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "357_nr9H3Nxa"
      },
      "source": [
        "df=pd.read_csv('https://raw.githubusercontent.com/Ali-Asgar-Lakdawala/NLP/main/Data/quora_questions.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "V3izEE1t3Nxb",
        "outputId": "2904ae05-6871-4108-eccf-de43d4a9192a"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question\n",
              "0  What is the step by step guide to invest in sh...\n",
              "1  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
              "2  How can I increase the speed of my internet co...\n",
              "3  Why am I mentally very lonely? How can I solve...\n",
              "4  Which one dissolve in water quikly sugar, salt..."
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NGMA0va3Nxd"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "#### Task: Use TF-IDF Vectorization to create a vectorized document term matrix. You may want to explore the max_df and min_df parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "mB6-LVqX3Nxe"
      },
      "source": [
        "from sklearn.decomposition import NMF,LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "zUHmwpGU3Nxf"
      },
      "source": [
        "Tfidf=TfidfVectorizer(max_df=0.95,min_df=2,stop_words='english')\n",
        "CV=CountVectorizer(max_df=0.95,min_df=2,stop_words='english')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdp0S3QX7t0X"
      },
      "source": [
        "tf_matrix=Tfidf.fit_transform(df['Question'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjNU2W6M7tvn"
      },
      "source": [
        "CV_matrix=CV.fit_transform(df['Question'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgBL3pGX3Nxh"
      },
      "source": [
        "# Non-negative Matrix Factorization\n",
        "\n",
        "#### TASK: Using Scikit-Learn create an instance of NMF with 20 expected components. (Use random_state=42).."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3UPFGifM3Nxi"
      },
      "source": [
        "LDA=LatentDirichletAllocation(n_components=20,random_state=42,n_jobs=-1,verbose=0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MhzOFN_k3Nxj"
      },
      "source": [
        "nmf=NMF(n_components=20,random_state=42,verbose=0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVbTSrxE95mX",
        "outputId": "e291cc5d-e464-472c-c4d7-c4d3585a1d09"
      },
      "source": [
        "LDA.fit(CV_matrix)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
              "                          evaluate_every=-1, learning_decay=0.7,\n",
              "                          learning_method='batch', learning_offset=10.0,\n",
              "                          max_doc_update_iter=100, max_iter=10,\n",
              "                          mean_change_tol=0.001, n_components=20, n_jobs=-1,\n",
              "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
              "                          total_samples=1000000.0, verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYLAf0Pj9Xzc",
        "outputId": "66d1d519-f000-4d3c-e738-0c9c00144df0"
      },
      "source": [
        "nmf.fit(tf_matrix)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
              "    n_components=20, random_state=42, shuffle=False, solver='cd', tol=0.0001,\n",
              "    verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TryeZXtv3Nxk"
      },
      "source": [
        "#### TASK: Print our the top 15 most common words for each of the 20 topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqZIpm6r9Lcb",
        "outputId": "f7603bf6-bdb3-4e73-cb67-fa480d1210ff"
      },
      "source": [
        "# top 15 words in each topic \n",
        "for index,value in enumerate(LDA.components_):\n",
        "  top_15_index = LDA.components_[index].argsort()[-15:]\n",
        "  print (f'top 15 words in topic {index}')\n",
        "  top_15_words=[]\n",
        "  for i in top_15_index:\n",
        "    top_15_words.append(CV.get_feature_names()[i])\n",
        "  print(top_15_words)\n",
        "  print('\\n')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 15 words in topic 0\n",
            "['use', 'source', 'sydney', 'open', 'sentence', 'good', 'does', 'career', 'google', 'change', 'history', 'india', 'service', 'used', 'best']\n",
            "\n",
            "\n",
            "top 15 words in topic 1\n",
            "['word', 'process', 'white', 'making', 'government', 'rupee', 'india', 'money', 'rs', 'black', 'english', '1000', 'notes', 'indian', '500']\n",
            "\n",
            "\n",
            "top 15 words in topic 2\n",
            "['alcohol', 'center', 'time', 'legal', 'compare', 'state', 'purpose', 'man', 'home', 'good', 'cost', 'really', 'average', 'india', 'does']\n",
            "\n",
            "\n",
            "top 15 words in topic 3\n",
            "['people', 'problem', 'facts', 'apple', 'series', 'looking', 'interesting', 'worth', 'exist', 'big', 'mind', 'tv', 'does', 'iphone', 'new']\n",
            "\n",
            "\n",
            "top 15 words in topic 4\n",
            "['students', 'mba', 'visa', 'canada', 'student', 'apply', 'car', 'jobs', 'facebook', 'email', 'differences', 'password', 'india', 'college', 'job']\n",
            "\n",
            "\n",
            "top 15 words in topic 5\n",
            "['number', 'relationship', 'look', 'countries', 'culture', 'support', 'pakistan', 'china', 'math', 'india', 'war', 'long', 'does', 'world', 'like']\n",
            "\n",
            "\n",
            "top 15 words in topic 6\n",
            "['study', 'university', 'visit', 'places', 'meaning', 'good', 'write', 'place', 'days', 'learning', 'live', 'lose', 'best', 'weight', 'start']\n",
            "\n",
            "\n",
            "top 15 words in topic 7\n",
            "['make', 'earn', 'english', 'president', 'movies', 'hillary', 'ways', 'clinton', 'india', 'improve', 'donald', 'money', 'online', 'trump', 'best']\n",
            "\n",
            "\n",
            "top 15 words in topic 8\n",
            "['social', 'number', 'website', 'effects', 'internet', 'whatsapp', 'using', 'thing', 'mobile', 'android', 'app', 'data', 'free', 'phone', 'use']\n",
            "\n",
            "\n",
            "top 15 words in topic 9\n",
            "['happy', 'child', 'makes', 'industry', 'wrong', 'stock', 'deal', 'known', 'india', 'age', 'fall', 'height', 'market', 'increase', 'important']\n",
            "\n",
            "\n",
            "top 15 words in topic 10\n",
            "['java', 'apps', 'design', 'employees', 'new', 'windows', 'learn', 'science', 'computer', 'going', 'language', 'day', 'programming', 'things', 'know']\n",
            "\n",
            "\n",
            "top 15 words in topic 11\n",
            "['hate', 'questions', 'god', 'google', 'tech', 'ask', 'software', 'mechanical', 'believe', 'quora', 'engineer', 'working', 'engineering', 'difference', 'people']\n",
            "\n",
            "\n",
            "top 15 words in topic 12\n",
            "['india', 'date', 'popular', 'indian', 'education', 'hack', 'books', 'read', 'years', 'school', 'movie', 'account', 'old', 'year', 'life']\n",
            "\n",
            "\n",
            "top 15 words in topic 13\n",
            "['friend', 'credit', 'worst', 'battle', 'money', 'happened', 'hotel', 'police', 'game', 'safe', 'tell', 'card', 'instagram', 'make', 'did']\n",
            "\n",
            "\n",
            "top 15 words in topic 14\n",
            "['chinese', 'created', 'air', 'happen', 'space', 'speed', 'dark', 'light', 'universe', 'com', 'human', 'does', 'earth', 'energy', 'think']\n",
            "\n",
            "\n",
            "top 15 words in topic 15\n",
            "['asked', 'answers', 'buy', 'ask', 'like', 'girls', 'make', 'youtube', 'answer', 'happens', 'best', 'business', 'questions', 'question', 'quora']\n",
            "\n",
            "\n",
            "top 15 words in topic 16\n",
            "['score', 'gate', 'jee', 'preparation', 'modi', 'iit', 'good', '2017', 'cat', '2016', 'united', 'class', 'states', 'exam', 'prepare']\n",
            "\n",
            "\n",
            "top 15 words in topic 17\n",
            "['problems', 'future', 'main', 'daily', 'common', 'biggest', 'law', 'living', 'die', 'parents', 'life', 'travel', 'water', 'time', 'possible']\n",
            "\n",
            "\n",
            "top 15 words in topic 18\n",
            "['reduce', 'good', 'self', 'fat', 'buy', 'books', 'eat', 'laptop', 'hair', 'men', 'women', 'book', 'learn', 'way', 'best']\n",
            "\n",
            "\n",
            "top 15 words in topic 19\n",
            "['girlfriend', 'guy', 'time', 'favorite', 'person', 'good', 'girl', 'sex', 'stop', 'love', 'feel', 'like', 'work', 'mean', 'does']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M77hBOGI9LYv",
        "outputId": "eb18541c-ca9b-4b4b-c1fd-6cd7ea67deb7"
      },
      "source": [
        "# top 15 words in each topic \n",
        "for index,value in enumerate(nmf.components_):\n",
        "  top_15_index = nmf.components_[index].argsort()[-15:]\n",
        "  print (f'top 15 words in topic {index}')\n",
        "  top_15_words=[]\n",
        "  for i in top_15_index:\n",
        "    top_15_words.append(Tfidf.get_feature_names()[i])\n",
        "  print(top_15_words)\n",
        "  print('\\n')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 15 words in topic 0\n",
            "['thing', 'read', 'place', 'visit', 'places', 'phone', 'buy', 'laptop', 'movie', 'ways', '2016', 'books', 'book', 'movies', 'best']\n",
            "\n",
            "\n",
            "top 15 words in topic 1\n",
            "['majors', 'recruit', 'sex', 'looking', 'differ', 'use', 'exist', 'really', 'compare', 'cost', 'long', 'feel', 'work', 'mean', 'does']\n",
            "\n",
            "\n",
            "top 15 words in topic 2\n",
            "['add', 'answered', 'needing', 'post', 'easily', 'improvement', 'delete', 'asked', 'google', 'answers', 'answer', 'ask', 'question', 'questions', 'quora']\n",
            "\n",
            "\n",
            "top 15 words in topic 3\n",
            "['using', 'website', 'investment', 'friends', 'black', 'internet', 'free', 'home', 'easy', 'youtube', 'ways', 'earn', 'online', 'make', 'money']\n",
            "\n",
            "\n",
            "top 15 words in topic 4\n",
            "['balance', 'earth', 'day', 'death', 'changed', 'live', 'want', 'change', 'moment', 'real', 'important', 'thing', 'meaning', 'purpose', 'life']\n",
            "\n",
            "\n",
            "top 15 words in topic 5\n",
            "['reservation', 'engineering', 'minister', 'president', 'company', 'china', 'business', 'country', 'olympics', 'available', 'job', 'spotify', 'war', 'pakistan', 'india']\n",
            "\n",
            "\n",
            "top 15 words in topic 6\n",
            "['beginners', 'online', 'english', 'book', 'did', 'hacking', 'want', 'python', 'languages', 'java', 'learning', 'start', 'language', 'programming', 'learn']\n",
            "\n",
            "\n",
            "top 15 words in topic 7\n",
            "['happen', 'presidency', 'think', 'presidential', '2016', 'vote', 'better', 'election', 'did', 'win', 'hillary', 'president', 'clinton', 'donald', 'trump']\n",
            "\n",
            "\n",
            "top 15 words in topic 8\n",
            "['russia', 'business', 'win', 'coming', 'countries', 'place', 'pakistan', 'happen', 'end', 'country', 'iii', 'start', 'did', 'war', 'world']\n",
            "\n",
            "\n",
            "top 15 words in topic 9\n",
            "['indian', 'companies', 'don', 'guy', 'men', 'culture', 'women', 'work', 'girls', 'live', 'girl', 'look', 'sex', 'feel', 'like']\n",
            "\n",
            "\n",
            "top 15 words in topic 10\n",
            "['ca', 'departments', 'positions', 'movies', 'songs', 'business', 'read', 'start', 'job', 'work', 'engineering', 'ways', 'bad', 'books', 'good']\n",
            "\n",
            "\n",
            "top 15 words in topic 11\n",
            "['money', 'modi', 'currency', 'economy', 'think', 'government', 'ban', 'banning', 'black', 'indian', 'rupee', 'rs', '1000', 'notes', '500']\n",
            "\n",
            "\n",
            "top 15 words in topic 12\n",
            "['blowing', 'resolutions', 'resolution', 'mind', 'likes', 'girl', '2017', 'year', 'don', 'employees', 'going', 'day', 'things', 'new', 'know']\n",
            "\n",
            "\n",
            "top 15 words in topic 13\n",
            "['aspects', 'fluent', 'skill', 'spoken', 'ways', 'language', 'fluently', 'speak', 'communication', 'pronunciation', 'speaking', 'writing', 'skills', 'improve', 'english']\n",
            "\n",
            "\n",
            "top 15 words in topic 14\n",
            "['diet', 'help', 'healthy', 'exercise', 'month', 'pounds', 'reduce', 'quickly', 'loss', 'fast', 'fat', 'ways', 'gain', 'lose', 'weight']\n",
            "\n",
            "\n",
            "top 15 words in topic 15\n",
            "['having', 'feel', 'long', 'spend', 'did', 'person', 'machine', 'movies', 'favorite', 'job', 'home', 'sex', 'possible', 'travel', 'time']\n",
            "\n",
            "\n",
            "top 15 words in topic 16\n",
            "['marriage', 'make', 'did', 'girlfriend', 'feel', 'tell', 'forget', 'really', 'friend', 'true', 'know', 'person', 'girl', 'fall', 'love']\n",
            "\n",
            "\n",
            "top 15 words in topic 17\n",
            "['easy', 'hack', 'prepare', 'quickest', 'facebook', 'increase', 'painless', 'instagram', 'account', 'best', 'commit', 'fastest', 'suicide', 'easiest', 'way']\n",
            "\n",
            "\n",
            "top 15 words in topic 18\n",
            "['web', 'java', 'scripting', 'phone', 'mechanical', 'better', 'job', 'use', 'account', 'data', 'software', 'science', 'computer', 'engineering', 'difference']\n",
            "\n",
            "\n",
            "top 15 words in topic 19\n",
            "['earth', 'blowing', 'stop', 'use', 'easily', 'mind', 'google', 'flat', 'questions', 'hate', 'believe', 'ask', 'don', 'think', 'people']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd4n4J6B3Nxl"
      },
      "source": [
        "#### TASK: Add a new column to the original quora dataframe that labels each question into one of the 20 topic categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjIIU89z3Nxl"
      },
      "source": [
        "topic_array_nmf=nmf.transform(tf_matrix)\n",
        "topic_array_lda=LDA.transform(CV_matrix)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fxNOuY253Nxm"
      },
      "source": [
        "final_topics=[]\n",
        "for i,v in enumerate(topic_array_nmf):\n",
        "  final_topics.append(topic_array_nmf[i].argmax())\n",
        "df['Topics_nmf']=pd.DataFrame(final_topics)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrCdGfjuDLnr"
      },
      "source": [
        "final_topics=[]\n",
        "for i,v in enumerate(topic_array_lda):\n",
        "  final_topics.append(topic_array_lda[i].argmax())\n",
        "df['Topics_lda']=pd.DataFrame(final_topics)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "1CbHbx1kDnSi",
        "outputId": "43f0259c-4dbe-4915-f522-5060260736c2"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Topics_nmf</th>\n",
              "      <th>Topics_lda</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>16</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>17</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>14</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question  Topics_nmf  Topics_lda\n",
              "0  What is the step by step guide to invest in sh...           5          16\n",
              "1  What is the story of Kohinoor (Koh-i-Noor) Dia...          16          17\n",
              "2  How can I increase the speed of my internet co...          17           8\n",
              "3  Why am I mentally very lonely? How can I solve...          11          10\n",
              "4  Which one dissolve in water quikly sugar, salt...          14          17"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuaFihdG3Nxm"
      },
      "source": [
        "# Great job!"
      ]
    }
  ]
}